{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7be42ee1-e23e-4976-a4a6-4b532bd07e89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformations:\n{}\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-7518137628170193>, line 46\u001B[0m\n",
       "\u001B[1;32m     41\u001B[0m df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mheader\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mTrue\u001B[39;00m)\u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minferSchema\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mTrue\u001B[39;00m)\u001B[38;5;241m.\u001B[39mcsv(\n",
       "\u001B[1;32m     42\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwasbs://mmix-blob-storage@mmixstorage.blob.core.windows.net/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00minput_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m     43\u001B[0m )\n",
       "\u001B[1;32m     45\u001B[0m \u001B[38;5;66;03m# Convert date column to DateType\u001B[39;00m\n",
       "\u001B[0;32m---> 46\u001B[0m df \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mwithColumn(date_col, F\u001B[38;5;241m.\u001B[39mto_timestamp(F\u001B[38;5;241m.\u001B[39mcol(date_col), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mM/d/yyyy H:mm\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
       "\u001B[1;32m     48\u001B[0m \u001B[38;5;66;03m# ====================\u001B[39;00m\n",
       "\u001B[1;32m     49\u001B[0m \u001B[38;5;66;03m# 4. Apply adstock + saturation\u001B[39;00m\n",
       "\u001B[1;32m     50\u001B[0m \u001B[38;5;66;03m# ====================\u001B[39;00m\n",
       "\u001B[1;32m     51\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m col, trans \u001B[38;5;129;01min\u001B[39;00m transformations\u001B[38;5;241m.\u001B[39mitems():\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     50\u001B[0m     )\n",
       "\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:6282\u001B[0m, in \u001B[0;36mDataFrame.withColumn\u001B[0;34m(self, colName, col)\u001B[0m\n",
       "\u001B[1;32m   6277\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(col, Column):\n",
       "\u001B[1;32m   6278\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n",
       "\u001B[1;32m   6279\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_COLUMN\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m   6280\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcol\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(col)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m},\n",
       "\u001B[1;32m   6281\u001B[0m     )\n",
       "\u001B[0;32m-> 6282\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jdf\u001B[38;5;241m.\u001B[39mwithColumn(colName, col\u001B[38;5;241m.\u001B[39m_jc), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n",
       "\u001B[1;32m   1356\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n",
       "\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:261\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    257\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    258\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    259\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    260\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 261\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m    262\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    263\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `` cannot be resolved. Did you mean one of the following? [`Date`, `Year`, `Month`, `Sales`, `DTC TV`]. SQLSTATE: 42703;\n",
       "'Project [HCP ID#23, Date#24, Month#25, Year#26, Sales#27, HCP Calls#28, HCP Calls Spend#29, HCP Samples#30, HCP Samples Spend#31, HCP Print#32, HCP Print Spend#33, DTC TV#34, DTC TV Spend#35, DTC Display#36, DTC Display Spend#37, 'to_timestamp(', M/d/yyyy H:mm) AS #53]\n",
       "+- Relation [HCP ID#23,Date#24,Month#25,Year#26,Sales#27,HCP Calls#28,HCP Calls Spend#29,HCP Samples#30,HCP Samples Spend#31,HCP Print#32,HCP Print Spend#33,DTC TV#34,DTC TV Spend#35,DTC Display#36,DTC Display Spend#37] csv\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {
        "date_col": {
         "defaultValue": "",
         "label": null,
         "name": "date_col",
         "options": {
          "autoCreated": null,
          "validationRegex": null,
          "widgetType": "text"
         },
         "widgetType": "text"
        },
        "group_col": {
         "defaultValue": "",
         "label": null,
         "name": "group_col",
         "options": {
          "autoCreated": null,
          "validationRegex": null,
          "widgetType": "text"
         },
         "widgetType": "text"
        },
        "input_path": {
         "defaultValue": "",
         "label": null,
         "name": "input_path",
         "options": {
          "autoCreated": null,
          "validationRegex": null,
          "widgetType": "text"
         },
         "widgetType": "text"
        },
        "output_path": {
         "defaultValue": "",
         "label": null,
         "name": "output_path",
         "options": {
          "autoCreated": null,
          "validationRegex": null,
          "widgetType": "text"
         },
         "widgetType": "text"
        },
        "target_col": {
         "defaultValue": "",
         "label": null,
         "name": "target_col",
         "options": {
          "autoCreated": null,
          "validationRegex": null,
          "widgetType": "text"
         },
         "widgetType": "text"
        },
        "transformations": {
         "defaultValue": "{}",
         "label": null,
         "name": "transformations",
         "options": {
          "autoCreated": null,
          "validationRegex": null,
          "widgetType": "text"
         },
         "widgetType": "text"
        }
       },
       "arguments": {
        "date_col": "",
        "group_col": "",
        "input_path": "",
        "output_path": "",
        "target_col": "",
        "transformations": "{}"
       },
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "AnalysisException",
        "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `` cannot be resolved. Did you mean one of the following? [`Date`, `Year`, `Month`, `Sales`, `DTC TV`]. SQLSTATE: 42703;\n'Project [HCP ID#23, Date#24, Month#25, Year#26, Sales#27, HCP Calls#28, HCP Calls Spend#29, HCP Samples#30, HCP Samples Spend#31, HCP Print#32, HCP Print Spend#33, DTC TV#34, DTC TV Spend#35, DTC Display#36, DTC Display Spend#37, 'to_timestamp(', M/d/yyyy H:mm) AS #53]\n+- Relation [HCP ID#23,Date#24,Month#25,Year#26,Sales#27,HCP Calls#28,HCP Calls Spend#29,HCP Samples#30,HCP Samples Spend#31,HCP Print#32,HCP Print Spend#33,DTC TV#34,DTC TV Spend#35,DTC Display#36,DTC Display Spend#37] csv\n"
       },
       "metadata": {
        "errorSummary": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `` cannot be resolved. Did you mean one of the following? [`Date`, `Year`, `Month`, `Sales`, `DTC TV`]. SQLSTATE: 42703"
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": "UNRESOLVED_COLUMN.WITH_SUGGESTION",
        "pysparkCallSite": null,
        "pysparkFragment": null,
        "pysparkSummary": null,
        "sqlState": "42703",
        "stackTrace": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
        "File \u001B[0;32m<command-7518137628170193>, line 46\u001B[0m\n\u001B[1;32m     41\u001B[0m df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mheader\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mTrue\u001B[39;00m)\u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minferSchema\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mTrue\u001B[39;00m)\u001B[38;5;241m.\u001B[39mcsv(\n\u001B[1;32m     42\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwasbs://mmix-blob-storage@mmixstorage.blob.core.windows.net/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00minput_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     43\u001B[0m )\n\u001B[1;32m     45\u001B[0m \u001B[38;5;66;03m# Convert date column to DateType\u001B[39;00m\n\u001B[0;32m---> 46\u001B[0m df \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mwithColumn(date_col, F\u001B[38;5;241m.\u001B[39mto_timestamp(F\u001B[38;5;241m.\u001B[39mcol(date_col), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mM/d/yyyy H:mm\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m     48\u001B[0m \u001B[38;5;66;03m# ====================\u001B[39;00m\n\u001B[1;32m     49\u001B[0m \u001B[38;5;66;03m# 4. Apply adstock + saturation\u001B[39;00m\n\u001B[1;32m     50\u001B[0m \u001B[38;5;66;03m# ====================\u001B[39;00m\n\u001B[1;32m     51\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m col, trans \u001B[38;5;129;01min\u001B[39;00m transformations\u001B[38;5;241m.\u001B[39mitems():\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     50\u001B[0m     )\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:6282\u001B[0m, in \u001B[0;36mDataFrame.withColumn\u001B[0;34m(self, colName, col)\u001B[0m\n\u001B[1;32m   6277\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(col, Column):\n\u001B[1;32m   6278\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n\u001B[1;32m   6279\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_COLUMN\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   6280\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcol\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(col)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m},\n\u001B[1;32m   6281\u001B[0m     )\n\u001B[0;32m-> 6282\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jdf\u001B[38;5;241m.\u001B[39mwithColumn(colName, col\u001B[38;5;241m.\u001B[39m_jc), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n",
        "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1356\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:261\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    257\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    258\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    259\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    260\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 261\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    262\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    263\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
        "\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `` cannot be resolved. Did you mean one of the following? [`Date`, `Year`, `Month`, `Sales`, `DTC TV`]. SQLSTATE: 42703;\n'Project [HCP ID#23, Date#24, Month#25, Year#26, Sales#27, HCP Calls#28, HCP Calls Spend#29, HCP Samples#30, HCP Samples Spend#31, HCP Print#32, HCP Print Spend#33, DTC TV#34, DTC TV Spend#35, DTC Display#36, DTC Display Spend#37, 'to_timestamp(', M/d/yyyy H:mm) AS #53]\n+- Relation [HCP ID#23,Date#24,Month#25,Year#26,Sales#27,HCP Calls#28,HCP Calls Spend#29,HCP Samples#30,HCP Samples Spend#31,HCP Print#32,HCP Print Spend#33,DTC TV#34,DTC TV Spend#35,DTC Display#36,DTC Display Spend#37] csv\n"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# ====================\n",
    "# 1. Set up credentials\n",
    "# ====================\n",
    "spark.conf.set(\n",
    "    \"fs.azure.account.key.mmixstorage.blob.core.windows.net\",\n",
    "    \"UZTHs33FPYTUvC9G51zk+DQQp/FWf31YOteoW+dEnKuprRgxvk53yS+IpEiLn1062IBpOyoKaXp4+AStRcA1Cw==\"\n",
    ")\n",
    "\n",
    "# ====================\n",
    "# 2. Get notebook parameters\n",
    "# ====================\n",
    "dbutils.widgets.text(\"input_path\", \"\")\n",
    "dbutils.widgets.text(\"output_path\", \"\")\n",
    "dbutils.widgets.text(\"group_col\", \"\")\n",
    "dbutils.widgets.text(\"date_col\", \"\")\n",
    "dbutils.widgets.text(\"target_col\", \"\")\n",
    "dbutils.widgets.text(\"transformations\", \"{}\")\n",
    "\n",
    "import json\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import DateType\n",
    "\n",
    "def sql_safe(colname):\n",
    "    return f\"`{colname}`\"\n",
    "\n",
    "# Load parameters\n",
    "input_path = dbutils.widgets.get(\"input_path\")\n",
    "output_path = dbutils.widgets.get(\"output_path\")\n",
    "group_col = dbutils.widgets.get(\"group_col\")\n",
    "date_col = dbutils.widgets.get(\"date_col\")\n",
    "target_col = dbutils.widgets.get(\"target_col\")\n",
    "transformations = json.loads(dbutils.widgets.get(\"transformations\"))\n",
    "\n",
    "print(\"Transformations:\")\n",
    "print(json.dumps(transformations, indent=2))\n",
    "\n",
    "# ====================\n",
    "# 3. Read input CSV\n",
    "# ====================\n",
    "df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\n",
    "    f\"wasbs://mmix-blob-storage@mmixstorage.blob.core.windows.net/{input_path}\"\n",
    ")\n",
    "\n",
    "# Convert date column to DateType\n",
    "df = df.withColumn(date_col, F.to_timestamp(F.col(date_col), \"M/d/yyyy H:mm\"))\n",
    "\n",
    "# ====================\n",
    "# 4. Apply adstock + saturation\n",
    "# ====================\n",
    "for col, trans in transformations.items():\n",
    "    lags = int(trans.get(\"lags\", 0))\n",
    "    decay = float(trans.get(\"decay\", 0.0))\n",
    "\n",
    "    transformed_col = f\"{col}_transformed\"\n",
    "\n",
    "    # ----- Apply Adstock -----\n",
    "    if lags > 0 and decay > 0:\n",
    "        for i in range(1, lags + 1):\n",
    "            df = df.withColumn(f\"{col}_lag{i}\", F.lag(col, i).over(Window.partitionBy(group_col).orderBy(date_col)))\n",
    "\n",
    "        expr = f\"COALESCE({sql_safe(col)}, 0)\"\n",
    "        for i in range(1, lags + 1):\n",
    "            expr += f\" + POWER({decay}, {i}) * COALESCE({sql_safe(col + '_lag' + str(i))}, 0)\"\n",
    "\n",
    "        df = df.withColumn(transformed_col, F.expr(f\"({expr})\"))\n",
    "        df = df.drop(*[f\"{col}_lag{i}\" for i in range(1, lags + 1)])\n",
    "    else:\n",
    "        df = df.withColumn(transformed_col, F.col(col))\n",
    "\n",
    "    # ----- Apply Saturation -----\n",
    "    saturation = trans.get(\"saturation\")\n",
    "    if saturation:\n",
    "        mode = saturation.get(\"mode\", \"power\")\n",
    "        k = float(saturation.get(\"k\", 1))\n",
    "\n",
    "        if mode == \"log\":\n",
    "            df = df.withColumn(transformed_col, F.pow(F.log1p(F.col(transformed_col)), k))\n",
    "        else:\n",
    "            df = df.withColumn(transformed_col, F.pow(F.col(transformed_col), k))\n",
    "\n",
    "    # ----- Overwrite original column -----\n",
    "    # df = df.drop(col).withColumnRenamed(transformed_col, col)\n",
    "\n",
    "# ====================\n",
    "# 5. Write transformed CSV to output path\n",
    "# ====================\n",
    "df.show()\n",
    "df.printSchema()\n",
    "\n",
    "df.write.mode(\"overwrite\").option(\"header\", True).csv(\n",
    "    f\"wasbs://mmix-blob-storage@mmixstorage.blob.core.windows.net/{output_path}\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "make_transformation",
   "widgets": {
    "date_col": {
     "currentValue": "",
     "nuid": "118029c7-a1c3-4ca7-a886-a2c269a46ec8",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "date_col",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "date_col",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "group_col": {
     "currentValue": "",
     "nuid": "4e2e0586-b4f6-42dd-8d85-26304b7fdca0",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "group_col",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "group_col",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "input_path": {
     "currentValue": "",
     "nuid": "31de2ecf-d73a-4aab-b6fd-90dfac46cffd",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "input_path",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "input_path",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "output_path": {
     "currentValue": "",
     "nuid": "1b094c42-edda-4bd7-b7b5-8c433b4ed7d0",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "output_path",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "output_path",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "target_col": {
     "currentValue": "",
     "nuid": "5d80653d-4af0-4553-a1c7-8e317d645136",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "target_col",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "target_col",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "transformations": {
     "currentValue": "{}",
     "nuid": "ab1ee1c2-a7b2-4f24-ad71-e186dd197e29",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "{}",
      "label": null,
      "name": "transformations",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "{}",
      "label": null,
      "name": "transformations",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}