{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0cf3345-696e-4beb-bc9f-49cc67d44083",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set up Azure Blob Storage Credentials\n",
    "spark.conf.set(\n",
    "    \"fs.azure.account.key.mmixstorage.blob.core.windows.net\",\n",
    "    \"UZTHs33FPYTUvC9G51zk+DQQp/FWf31YOteoW+dEnKuprRgxvk53yS+IpEiLn1062IBpOyoKaXp4+AStRcA1Cw==\"\n",
    ")\n",
    "\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Retrieve widget inputs\n",
    "dbutils.widgets.text(\"file_urls\", \"\")\n",
    "dbutils.widgets.text(\"merge_strategy\", \"\")\n",
    "dbutils.widgets.text(\"column_mappings\", \"\")\n",
    "dbutils.widgets.text(\"join_keys\", \"\")\n",
    "dbutils.widgets.text(\"join_type\", \"\")\n",
    "dbutils.widgets.text(\"output_filename\", \"\")\n",
    "#dbutils.widgets.text(\"drop_duplicates\", \"\")\n",
    "\n",
    "# Parse widget parameters and clean strings\n",
    "file_urls = json.loads(dbutils.widgets.get(\"file_urls\"))\n",
    "merge_strategy = dbutils.widgets.get(\"merge_strategy\").strip('\"').lower()\n",
    "column_mappings = json.loads(dbutils.widgets.get(\"column_mappings\"))\n",
    "try:\n",
    "    join_keys = json.loads(dbutils.widgets.get(\"join_keys\"))\n",
    "    if not isinstance(join_keys, list):\n",
    "        join_keys = [join_keys]\n",
    "except Exception:\n",
    "    raise ValueError(\"Join key must be a valid JSON list of column names.\")\n",
    "\n",
    "join_type = dbutils.widgets.get(\"join_type\").strip('\"')\n",
    "output_filename = dbutils.widgets.get(\"output_filename\").strip('\"')\n",
    "\n",
    "# Define base paths and add timestamp\n",
    "# timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "base_blob_url = \"wasbs://pre-processing@mmixstorage.blob.core.windows.net\"\n",
    "output_csv_dir = f\"{base_blob_url}/correct_csv_outputs/{output_filename}\"\n",
    "sample_csv_dir = f\"{base_blob_url}/correct_csv_outputs/{output_filename}_sample\"\n",
    "\n",
    "# Convert HTTPS URLs to WASBS Paths\n",
    "def convert_to_wasbs(url):\n",
    "    https_prefix = \"https://mmixstorage.blob.core.windows.net/pre-processing/\"\n",
    "    return url.replace(https_prefix, f\"{base_blob_url}/\")\n",
    "\n",
    "file_urls = [convert_to_wasbs(url) for url in file_urls]\n",
    "\n",
    "# Read CSV Files and Apply Column Renaming\n",
    "dfs = []\n",
    "for url, mapping in zip(file_urls, column_mappings):\n",
    "    df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(url)\n",
    "    for old_col, new_col in mapping.items():\n",
    "        df = df.withColumnRenamed(old_col, new_col)\n",
    "\n",
    "    # Filter to only keep renamed columns (values from mapping)\n",
    "    df = df.select([col_name for col_name in mapping.values()])\n",
    "    \n",
    "    dfs.append(df)\n",
    "\n",
    "# Merge DataFrames\n",
    "if merge_strategy in [\"vertical\", \"vertical stack\"]:\n",
    "    merged_df = dfs[0]\n",
    "    for df in dfs[1:]:\n",
    "        merged_df = merged_df.unionByName(df, allowMissingColumns=True)\n",
    "elif merge_strategy in [\"horizontal\", \"horizontal join\"]:\n",
    "    if not join_keys:\n",
    "        raise ValueError(\"Join key must be provided for horizontal joins.\")\n",
    "    merged_df = dfs[0]\n",
    "    for df in dfs[1:]:\n",
    "        merged_df = merged_df.join(df, on=join_keys, how=join_type)\n",
    "else:\n",
    "    raise ValueError(\"Invalid merge strategy. Choose 'vertical' or 'horizontal'.\")\n",
    "\n",
    "# Remove rows with nulls in identifier columns and drop duplicates\n",
    "if join_keys:\n",
    "    for col_name in join_keys:\n",
    "        if col_name in merged_df.columns:\n",
    "            merged_df = merged_df.filter(col(col_name).isNotNull())\n",
    "else:\n",
    "    print(\"⚠️ No join_keys provided — skipping null identifier filtering.\")\n",
    "\n",
    "merged_df = merged_df.dropDuplicates()\n",
    "\n",
    "# # Remove Duplicates\n",
    "# merged_df = merged_df.dropDuplicates()\n",
    "\n",
    "# Clean Existing Output Folders If Any\n",
    "dbutils.fs.rm(output_csv_dir, recurse=True)\n",
    "dbutils.fs.rm(sample_csv_dir, recurse=True)\n",
    "\n",
    "# Write Full Merged Output\n",
    "merged_df.coalesce(1).write.option(\"header\", True).mode(\"overwrite\").csv(output_csv_dir)\n",
    "\n",
    "# Write Sample Output (First 100 Rows)\n",
    "merged_df.limit(100).coalesce(1).write.option(\"header\", True).mode(\"overwrite\").csv(sample_csv_dir)\n",
    "\n",
    "print(\"✅ Processing and file writes completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8fc7eef4-20b8-49e5-ad5f-b6614cdf3693",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "merge_tables_advanced",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}