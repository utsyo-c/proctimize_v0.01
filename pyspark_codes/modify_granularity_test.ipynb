{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b5f7486-5480-45e0-8c45-3b9c5deb111b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.functions import col, when, expr, trunc, date_format, weekofyear\n",
    "from pyspark.sql.functions import last_day, dayofweek, date_sub\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql import functions as F\n",
    "import json\n",
    "\n",
    "def last_week_apportion_spark(df, date_col_name, kpi_col_list, work_days):\n",
    "    df = df.withColumn(\"year\", F.year(F.col(date_col_name))) \\\n",
    "           .withColumn(\"month\", F.month(F.col(date_col_name))) \\\n",
    "           .withColumn(\"last_day_of_month\", F.last_day(F.col(date_col_name)))\n",
    "\n",
    "    if work_days == 5:\n",
    "        df = df.withColumn(\n",
    "            \"last_working_date\",\n",
    "            F.when(F.dayofweek(F.col(\"last_day_of_month\")) == 7, F.date_sub(F.col(\"last_day_of_month\"), 1))  # Saturday\n",
    "             .when(F.dayofweek(F.col(\"last_day_of_month\")) == 1, F.date_sub(F.col(\"last_day_of_month\"), 2))  # Sunday\n",
    "             .otherwise(F.col(\"last_day_of_month\"))\n",
    "        )\n",
    "    else:\n",
    "        df = df.withColumn(\"last_working_date\", F.col(\"last_day_of_month\"))\n",
    "\n",
    "    df = df.withColumn(\"day_diff\", F.datediff(F.col(\"last_working_date\"), F.col(date_col_name)) + 1)\n",
    "\n",
    "    for kpi in kpi_col_list:\n",
    "        adj_kpi = f\"adjusted_{kpi}\"\n",
    "        df = df.withColumn(\n",
    "            adj_kpi,\n",
    "            F.when(F.col(\"day_diff\") < work_days, ((work_days - F.col(\"day_diff\")) / work_days) * F.col(kpi)).otherwise(F.lit(0))\n",
    "        )\n",
    "        df = df.withColumn(kpi, F.col(kpi) - F.col(adj_kpi))\n",
    "\n",
    "    new_rows = df\n",
    "    for kpi in kpi_col_list:\n",
    "        new_rows = new_rows.withColumn(kpi, F.col(f\"adjusted_{kpi}\"))\n",
    "\n",
    "    new_rows = new_rows.withColumn(date_col_name, F.add_months(F.col(date_col_name), 1))\n",
    "    combined_df = df.unionByName(new_rows)\n",
    "\n",
    "    drop_cols = [\"year\", \"month\", \"last_day_of_month\", \"last_working_date\", \"day_diff\"] + [f\"adjusted_{k}\" for k in kpi_col_list]\n",
    "    combined_df = combined_df.drop(*drop_cols)\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "\n",
    "def modify_granularity_spark(\n",
    "    df,\n",
    "    geo_column,\n",
    "    date_column,\n",
    "    granularity_level_df,\n",
    "    granularity_level_user_input,\n",
    "    work_days,\n",
    "    numerical_config_dict,\n",
    "    categorical_config_dict\n",
    "):\n",
    "    def get_agg_exprs(numerical_dict, categorical_dict):\n",
    "        agg_exprs = []\n",
    "\n",
    "        for col_name, operation in numerical_dict.items():\n",
    "            if operation == \"sum\":\n",
    "                agg_exprs.append(F.sum(F.col(col_name)).alias(col_name))\n",
    "            elif operation == \"average\":\n",
    "                agg_exprs.append(F.avg(F.col(col_name)).alias(col_name))\n",
    "            elif operation == \"min\":\n",
    "                agg_exprs.append(F.min(F.col(col_name)).alias(col_name))\n",
    "            elif operation == \"max\":\n",
    "                agg_exprs.append(F.max(F.col(col_name)).alias(col_name))\n",
    "            elif operation == \"product\":\n",
    "                agg_exprs.append(F.expr(f\"aggregate(collect_list({col_name}), 1D, (acc, x) -> acc * x)\").alias(col_name))\n",
    "\n",
    "        for col_name, operation in categorical_dict.items():\n",
    "            if operation == \"count\":\n",
    "                agg_exprs.append(F.count(F.col(col_name)).alias(f\"{col_name}_count\"))\n",
    "            elif operation == \"distinct count\":\n",
    "                agg_exprs.append(F.countDistinct(F.col(col_name)).alias(f\"{col_name}_count_distinct\"))\n",
    "            # Pivot is ignored for now\n",
    "\n",
    "        return agg_exprs\n",
    "\n",
    "    output_columns = [geo_column] + [granularity_level_user_input.lower() + \"_date\"] + list(numerical_config_dict.keys()) + \\\n",
    "                     [f\"{k}_count\" if v == \"count\" else f\"{k}_count_distinct\" for k, v in categorical_config_dict.items() if v in [\"count\", \"count distinct\"]]\n",
    "\n",
    "    if granularity_level_df == granularity_level_user_input:\n",
    "        selected_cols = [geo_column, date_column] + list(numerical_config_dict.keys()) + list(categorical_config_dict.keys())\n",
    "        return df.select(*selected_cols), date_column\n",
    "\n",
    "    if granularity_level_df == \"Daily\" and granularity_level_user_input == \"Weekly\":\n",
    "        df = df.withColumn(\"week_date\", F.date_sub(F.col(date_column), F.dayofweek(F.col(date_column)) - 1))\n",
    "        df = df.groupBy(geo_column, \"week_date\").agg(*get_agg_exprs(numerical_config_dict, categorical_config_dict)).orderBy(geo_column, \"week_date\")\n",
    "        return df, \"week_date\"\n",
    "\n",
    "    elif granularity_level_df == \"Daily\" and granularity_level_user_input == \"Monthly\":\n",
    "        df = df.withColumn(\"month_date\", F.date_format(F.col(date_column), \"yyyy-MM-01\"))\n",
    "        df = df.groupBy(geo_column, \"month_date\").agg(*get_agg_exprs(numerical_config_dict, categorical_config_dict)).orderBy(geo_column, \"month_date\")\n",
    "        return df, \"month_date\"\n",
    "\n",
    "    elif granularity_level_df == \"Weekly\" and granularity_level_user_input == \"Monthly\":\n",
    "        df = last_week_apportion_spark(df, date_column, list(numerical_config_dict.keys()), work_days)\n",
    "        df = df.withColumn(\"month_date\", F.date_format(F.col(date_column), \"yyyy-MM-01\"))\n",
    "        df = df.groupBy(geo_column, \"month_date\").agg(*get_agg_exprs(numerical_config_dict, categorical_config_dict)).orderBy(geo_column, \"month_date\")\n",
    "        return df, \"month_date\"\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported granularity transformation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2410ed79-7500-4355-933b-308d07387bb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set Spark config\n",
    "spark.conf.set(\n",
    "    \"fs.azure.account.key.mmixstorage.blob.core.windows.net\",\n",
    "    \"UZTHs33FPYTUvC9G51zk+DQQp/FWf31YOteoW+dEnKuprRgxvk53yS+IpEiLn1062IBpOyoKaXp4+AStRcA1Cw==\"\n",
    ")\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "dbutils.widgets.removeAll()\n",
    "\n",
    "\n",
    "# Get widget inputs\n",
    "dbutils.widgets.text(\"input_path\", \"\")\n",
    "dbutils.widgets.text(\"output_path\", \"\")\n",
    "dbutils.widgets.text(\"geo_col\", \"\")\n",
    "dbutils.widgets.text(\"date_col\", \"\")\n",
    "dbutils.widgets.text(\"granularity_level_df\", \"\")\n",
    "dbutils.widgets.text(\"granularity_level_user_input\", \"\")\n",
    "dbutils.widgets.text(\"work_days\", \"\")\n",
    "dbutils.widgets.text(\"numerical_config_dict\", \"{}\")\n",
    "dbutils.widgets.text(\"categorical_config_dict\", \"{}\")\n",
    "\n",
    "input_path = dbutils.widgets.get(\"input_path\")\n",
    "output_path = dbutils.widgets.get(\"output_path\")\n",
    "geo_col = dbutils.widgets.get(\"geo_col\")\n",
    "date_col = dbutils.widgets.get(\"date_col\")\n",
    "granularity_level_df = dbutils.widgets.get(\"granularity_level_df\")\n",
    "granularity_level_user_input = dbutils.widgets.get(\"granularity_level_user_input\")\n",
    "work_days = int(dbutils.widgets.get(\"work_days\"))\n",
    "numerical_config_dict = json.loads(dbutils.widgets.get(\"numerical_config_dict\"))\n",
    "categorical_config_dict = json.loads(dbutils.widgets.get(\"categorical_config_dict\"))\n",
    "\n",
    "base_blob_url = \"wasbs://pre-processing@mmixstorage.blob.core.windows.net\"\n",
    "\n",
    "# Convert HTTPS URLs to WASBS Paths\n",
    "def convert_to_wasbs(url):\n",
    "    https_prefix = \"https://mmixstorage.blob.core.windows.net/pre-processing/\"\n",
    "    return url.replace(https_prefix, f\"{base_blob_url}/\")\n",
    "\n",
    "input_path  = convert_to_wasbs(input_path)\n",
    "output_path = convert_to_wasbs(output_path)\n",
    "\n",
    "# Read input\n",
    "df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(input_path)\n",
    "\n",
    "#df = df.withColumn(date_col, F.to_date(F.col(date_col), \"MM/dd/yyyy\"))\n",
    "df = df.filter(F.col(date_col).isNotNull())\n",
    "df.printSchema()\n",
    "\n",
    "# Modify granularity\n",
    "modified_granular_df, new_date_col = modify_granularity_spark(\n",
    "    df,\n",
    "    geo_col,\n",
    "    date_col,\n",
    "    granularity_level_df,\n",
    "    granularity_level_user_input,\n",
    "    work_days,\n",
    "    numerical_config_dict,\n",
    "    categorical_config_dict\n",
    ")\n",
    "\n",
    "# Save result\n",
    "display(modified_granular_df)\n",
    "modified_granular_df.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(output_path)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "modify_granularity_test",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}