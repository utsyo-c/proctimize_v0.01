{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "146db124-da2a-4529-9841-8498de8d6b1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# ====================\n",
    "# 1. Set up credentials\n",
    "# ====================\n",
    "spark.conf.set(\n",
    "    \"fs.azure.account.key.mmixstorage.blob.core.windows.net\",\n",
    "    \"UZTHs33FPYTUvC9G51zk+DQQp/FWf31YOteoW+dEnKuprRgxvk53yS+IpEiLn1062IBpOyoKaXp4+AStRcA1Cw==\"\n",
    ")\n",
    "\n",
    "# Set legacy time parser policy to handle 'MM/dd/yyyy' format\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "# ====================\n",
    "# 2. Get notebook parameters\n",
    "# ====================\n",
    "dbutils.widgets.text(\"input_path\", \"\")\n",
    "dbutils.widgets.text(\"output_path\", \"\")\n",
    "dbutils.widgets.text(\"date_col\", \"\")\n",
    "\n",
    "input_path = dbutils.widgets.get(\"input_path\")\n",
    "output_path = dbutils.widgets.get(\"output_path\")\n",
    "date_col = dbutils.widgets.get(\"date_col\")\n",
    "\n",
    "base_blob_url = \"wasbs://pre-processing@mmixstorage.blob.core.windows.net\"\n",
    "\n",
    "# Convert HTTPS URLs to WASBS Paths\n",
    "def convert_to_wasbs(url):\n",
    "    https_prefix = \"https://mmixstorage.blob.core.windows.net/pre-processing/\"\n",
    "    return url.replace(https_prefix, f\"{base_blob_url}/\")\n",
    "\n",
    "input_path = convert_to_wasbs(input_path)\n",
    "output_path = convert_to_wasbs(output_path)\n",
    "\n",
    "# ====================\n",
    "# 3. Read input CSV\n",
    "# ====================\n",
    "df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(input_path)\n",
    "\n",
    "df.show(20)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ====================\n",
    "# 4. Define granularity detection function\n",
    "# ====================\n",
    "from pyspark.sql.functions import col, to_date, datediff, lag\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "def detect_date_granularity_spark(df_spark, date_column):\n",
    "\n",
    "    #df = df_spark.withColumn(date_column, to_date(col(date_column), 'MM/dd/yyyy'))\n",
    "    df = df_spark \n",
    "    \n",
    "    df = df.dropna(subset=[date_column]).dropDuplicates([date_column])\n",
    "    df = df.orderBy(col(date_column))\n",
    "\n",
    "    df.select(date_column).show(10)  # Show first 10 rows of the date column\n",
    "\n",
    "    \n",
    "    w = Window().orderBy(date_column)\n",
    "    df = df.withColumn(\"date_diff\", \n",
    "    datediff(col(date_column), lag(date_column).over(w)))\n",
    "    df_diff = df.filter(col(\"date_diff\").isNotNull())\n",
    "    \n",
    "    mode_df = df_diff.groupBy(\"date_diff\").count().orderBy(col(\"count\").desc()).limit(1)\n",
    "    \n",
    "    mode_rows = mode_df.collect()\n",
    "\n",
    "    mode_df.show()\n",
    "    if not mode_rows:\n",
    "        return None  # Return None if there's no mode value\n",
    "    else:\n",
    "        most_common_diff = mode_rows[0][\"date_diff\"]\n",
    "        return most_common_diff\n",
    "    \n",
    "\n",
    "\n",
    "# ====================\n",
    "# 5. Detect granularity\n",
    "# ====================\n",
    "most_common_diff = detect_date_granularity_spark(df, date_col)\n",
    "\n",
    "# ====================\n",
    "# 6. Determine granularity\n",
    "# ====================\n",
    "if most_common_diff is None:\n",
    "    granularity = \"Insufficient Data\"\n",
    "elif most_common_diff == 1:\n",
    "    granularity = \"Daily\"\n",
    "elif most_common_diff == 7:\n",
    "    granularity = \"Weekly\"\n",
    "elif most_common_diff in [28, 29, 30, 31]:\n",
    "    granularity = \"Monthly\"\n",
    "elif most_common_diff >= 365:\n",
    "    granularity = \"Yearly\"\n",
    "else:\n",
    "    granularity = \"Irregular\"\n",
    "\n",
    "# ====================\n",
    "# 7. Write result to output path as a single-row CSV\n",
    "# ====================\n",
    "result_df = spark.createDataFrame([(granularity,)], [\"granularity\"])\n",
    "result_df.write.mode(\"overwrite\").option(\"header\", True).csv(output_path)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "detect_granularity_test",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}